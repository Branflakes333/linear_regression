\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}


\title{Linear Regression Analysis - HW1}
\author{Brandon Miner}
\date{\today}

\begin{document}
\maketitle 

\begin{enumerate}
    \item 
    False because there is no assumption made for Linear Regression on the distribution of the outcome. To elaborate, we only make an assumption on the distribution of the error $\epsilon_i$ in the classical case. Since $$Y_i=\beta_0+\beta_1X_i+\epsilon_i$$
    And of the variables on the right side of the equation, only $\epsilon_i$ is a random variable, $Y_i$ has the distribution of the transformation of the random error. Therefore, $Y_i$ can have any distribution because the error doesn't have an assumed distribution (in the standard case).
    
    \item 
    We are given in the book
    $$L(\beta_0,\beta_1,\sigma^2)=\frac{1}{(2\pi\sigma)^{\frac{n}{2}}}e^{\frac{-\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}}$$

    We denote the log-likelihood function as $\ell(\beta_0,\beta_1,\sigma^2):=\log(L(\beta_0,\beta_1,\sigma^2))$.
    Hence,
    $$\ell(\beta_0,\beta_1,\sigma^2)=\log(1)-\frac{n}{2}\log(2\pi\sigma^2)-\frac{-\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}$$
    Recall that in LS $$Q(\beta_0,\beta_1)=\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2$$
    Therefore,
    $$\ell(\beta_0,\beta_1,\sigma^2)=-\frac{n}{2}\log(2\pi\sigma^2)-\frac{Q(\beta_0,\beta_1)}{2\sigma^2}$$
    Since maximizing $\ell(\beta_0,\beta_1,\sigma^2)$ with respect to $\beta_0,\beta_1$ is equivalent to minimizing $Q(\beta_0, \beta_1)$, we know
    $$\hat\beta_0^{(MLE)}=\hat\beta_0^{(LS)}$$
    $$\hat\beta_1^{(MLE)}=\hat\beta_1^{(LS)}$$
    
    The MLE of $\sigma^2$ can be derived by taking the partial derivative with respect to $\sigma^2$
    $$\frac{\partial\ell(\beta_0,\beta_1,\sigma^2)}{\partial\sigma^2}=-\frac{n}{2}\cdot\frac{1}{2\pi\sigma^2}\cdot(2\pi)-(\frac{Q(\beta_0,\beta_1)}{2})(-1)(\sigma^2)^{-2}$$
    We simplify and set to zero:
    $$-\frac{n}{2\sigma^2}+\frac{Q(\beta_0,\beta_1)}{2}\cdot\frac{1}{\sigma^4}\stackrel{!}{=}0$$

    $$\frac{n}{2\sigma^2}=\frac{Q(\beta_0,\beta_1)}{2\sigma^4}$$

    $$2n\sigma^4=2\sigma^2Q(\beta_0,\beta_1)$$
    $$$$
    
    $$\sigma^2=\frac{Q(\beta_0,\beta_1)}{n}$$
    Thus,
    $$\hat\sigma^2_{(MLE)}=\frac{\sum^n_{i=1}(y_i-\hat\beta_0^{(MLE)}-\hat\beta_1^{(MLE)}x_i)^2}{n}$$
    
    \item 
    Proof:
    Given $$\hat\beta_1=\frac{\sum_{i=1}^n(x_iy_i)-n\bar x\bar y}{\sum_{i=1}^n(x_i^2)-n\bar x^2}$$
    We want to show 
    $$\text{(a)}\space\sum_{i=1}^n(x_iy_i)-n\bar x\bar y=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)$$
    
    \begin{center}
        \&
    \end{center}
    $$\text{(b)}\space\sum_{i=1}^nx^2_i-n\bar x^2=\sum_{i=1}^n(x_i-\bar x)^2$$

    \begin{enumerate}
        \item 
            Starting with the right-hand side, we multiply out into a quadratic:
            $$\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)=\sum_{i=1}^n(x_iy_i-x_i\bar y-y_i\bar x+\bar x\bar y)$$
            Distributing the summation:
            $$=\sum_{i=1}^n(x_iy_i)-\sum_{i=1}^n(x_i\bar y)-\sum_{i=1}^n(y_i\bar x)+\sum_{i=1}^n(\bar x\bar y)$$

            $$=\sum_{i=1}^n(x_iy_i)-
            \bar y\sum_{i=1}^n(x_i)-
            \bar x\sum_{i=1}^n(y_i)+
            n\bar x\bar y$$

            $$=\sum_{i=1}^n(x_iy_i)-
            \bar y(n\bar x)-
            \bar x(n\bar y)+
            n\bar x\bar y$$

            Combining the right three terms we see:
            
            $$=\sum_{i=1}^n(x_iy_i)-
            n\bar x\bar y$$
            As we desired.
        
        \item
            Starting with the right-hand side, we multiply out into a quadratic:
            $$\sum_{i=1}^n(x_i-\bar x)^2=\sum_{i=1}^n(x_i^2-2\bar x x_i+\bar x^2)$$
            Distributing the summation:
            $$=\sum_{i=1}^n(x_i^2)-2\bar x \sum_{i=1}^n(x_i)+n\bar x^2$$
            
            $$=\sum_{i=1}^n(x_i^2)-2\bar x (n\bar x)+n\bar x^2$$

            $$=\sum_{i=1}^n(x_i^2)-n\bar x^2$$
            As we desired.
    \end{enumerate}

        We have shown function (a) and (b) are equivalent to $SSXY$ and $SSX$ respectively.
        Hence, $$\hat\beta_1=\frac{SSXY}{SSX}\space$$
    
    \item

    $$E(e_i)=E(y_i-\hat y_i)=E(y_i)-E(\hat y_i)$$
    Recall that $y_i$ is a known/fixed variable, and that $\hat y_i$ is an unbiased estimator.
    Therefore,
    $$E(e_i)=y_i-y_i=0$$
    As desired.
    
    \item
    \begin{enumerate}
        \item 
        
        No because $E(y_i)=\bar y$ which know is equal to $\beta_0+\beta_1\bar x$. The equation of interest not only has the term $x_i$ instead of $\bar x$, but most importantly $\epsilon_i$ in the equation of interest is incorrect because $E(\epsilon_i)=0$.
        
        \item
        $Y$ is normal because it is a transformation of the random variable $\epsilon_i\sim N(\mu=0,\sigma^2=9)$. The parameters at each point $x=1,2,4$ is $N(6,9),N(10,9),N(18,9)$ respectively.
    \end{enumerate}
    
    \item
    \begin{enumerate}
        \item 
        Note: "$\%$" is the unit of rating.
        
        For every single digit increase in critic/Tomatometer rating, audience rating is predicted to increase by $0.4461\%$.

        If the critic/Tomatometer rating is $0\%$, the audience rating is $34.5089\%$.
        \item
        $38.0777$
        \item
        The critics appear to be much harsher judges of majority of films (films critics rate approximately $\le60\textnormal{\%}$) than the audience.
        \item
        $72.8735\%$
        \item
        The audience appears to be much harsher judges of good films (films critics rate approximately $\ge60\textnormal{\%}$) than critics.
        \item
        It appears the audience tends to have opinions averaged to be between $34\%$ and $75\%$ give or take. This makes sense due to the audience rating being the average of a population magnitudes larger than that of the critics. Overall, audiences seem to be nicer judges of films, but most people are hesitant to rate films extremely highly, and so the audience score drops of after $60\%$.
        \item
        $$X=34.5089+0.4461X\Rightarrow(1-0.4461)X=34.5089$$
        $$X=62.301679$$
        Or more generally 
        $$\frac{\beta_0}{1-\beta_1}$$
    \end{enumerate}
    
\end{enumerate}

\end{document}
